#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Abstract
Supervised dimensionality reduction has emerged as an important theme in
 the last decade.
 Despite the plethora of models and formulations, there is a lack of a simple
 model which aims to project the set of patterns into a space defined by
 the classes (or categories).
 To this end, we set up a model in which each class is represented as a
 1D subspace of the vector space formed by the features.
 Assuming the set of classes does not exceed the cardinality of the features,
 the model results in multiclass supervised learning in which the features
 of each class are projected into the class subspace.
 Class discrimination is automatically guaranteed via the imposition of
 orthogonality of the 1D class subspaces.
 The resulting optimization problem—formulated as the minimization of a
 sum of quadratic functions on a Stiefel manifold—while being non-convex
 (due to the constraints), nevertheless has a structure for which we can
 identify when we have reached a global minimum.
 After formulating a version with standard inner products, we extend the
 formulation to reproducing kernel Hilbert spaces in a straightforward manner.
 The optimization approach also extends in a similar fashion to the kernel
 version.
 Results and comparisons with the multiclass Fisher linear (and kernel)
 discriminants and principal component analysis (linear and kernel) showcase
 the relative merits of this approach to dimensionality reduction.
 
\end_layout

\end_body
\end_document
