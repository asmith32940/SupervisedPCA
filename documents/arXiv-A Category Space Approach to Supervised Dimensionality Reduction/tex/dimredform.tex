
\section{Dimensionality Reduction using a Category Space Formulation\label{Category-Vector-Space}}

\subsection{Maximizing the square of the inner product}

The principal goal of this paper is a new form of supervised dimensionality
reduction. Specifically, when we seek to marry principal component
analysis with supervised learning, by far the simplest synthesis is
category space dimensionality reduction with orthogonal class vectors.
Assume the existence of a feature space with each feature vector $x_{i}\in\mathbf{R}^{D}$.
Our goal is to perform supervised dimensionality reduction by reducing
the number of feature dimensions from $D$ to $K$ where $K\leq D$.
Here $K$ is the number of classes and the first simplifying assumption
made in this work is that we will represent the category space using
$K$ \emph{orthonormal} basis vectors $\left\{ w_{k}\right\} $ together
with an \emph{origin} $x_{0}\in\mathbf{R}^{D}$. The second assumption
we make is that each feature vector $x_{i}$ should have a large magnitude
inner product with its assigned class. From the orthonormality constraint
above, this automatically implies a small magnitude inner product
with all other weight vectors. A \emph{candidate objective function}
and constraints following the above considerations is

\begin{equation}
E(W)=-\frac{1}{2}\sum_{k=1}^{K}\sum_{i_{k}\in C_{k}}\left[w_{k}^{T}\left(x_{i_{k}}-x_{0}\right)\right]^{2}\label{eq:innerprodmax}
\end{equation}
and 

\begin{equation}
w_{k}^{T}w_{l}=\left\{ \begin{array}{cc}
1, & k=l\\
0, & k\neq l
\end{array}\right.\label{eq:wkwlinner}
\end{equation}
respectively. In (\ref{eq:innerprodmax}), $W=\left[w_{1},w_{2},\ldots,w_{K}\right]$.
Note that we have referred to this as a candidate objective function
for two reasons. First, the origin $x_{0}$ is still unspecified and
we cannot obviously minimize (\ref{eq:innerprodmax}) w.r.t. $x_{0}$
as the minimum value is not bounded from below. Second, it is not
clear why we cannot use the absolute value or other symmetric functions
of the inner product. Both these issues are addressed later in this
work. At present, we resolve the origin issue by setting $x_{0}$
to the centroid of all the feature vectors (with this choice getting
a principled justification below).

The objective function in (\ref{eq:innerprodmax}) is the negative
of a quadratic function. Since the function $-x^{2}$ is concave,
it admits a Legendre transform-based majorization \citep{yuille2003concave}
using the tangent of the function. That is, we propose to replace
objective functions of the form $-\frac{1}{2}x^{2}$ with $\min_{y}-xy+\frac{1}{2}y^{2}$
which can quickly checked to be valid for an unconstrained auxiliary
variable $y$. Note that this transformation yields a linear objective
function w.r.t. $x$ which is to be expected from the geometric interpretation
of a tangent. 

Consider the following Legendre transformation of the objective function
in (\ref{eq:innerprodmax}). The new objective function is 

\begin{equation}
E_{\mathrm{quad}}(W,Z)=\sum_{k=1}^{K}\sum_{i_{k}\in C_{k}}\left[z_{ki_{k}}\left(-w_{k}^{T}x_{i_{k}}+w_{k}^{T}x_{0}\right)+\frac{1}{2}z_{ki_{k}}^{2}\right]\label{eq:EWZ}
\end{equation}
where $Z=\left\{ z_{ki_{k}}|k\in\left\{ 1,\ldots,K\right\} ,i_{k}\in\left\{ 1,\ldots,|C_{k}|\right\} \right\} $.
Now consider this to be an objective function over $x_{0}$ as well.
In order to avoid minima at negative infinity, we require additional
constraints. One such constraint (and perhaps not the only one) is
of the form $\sum_{i_{k}\in C_{k}}z_{ki_{k}}=0,\forall k$. When this
constraint is imposed, we obtain a new objective function 

\begin{equation}
E_{\mathrm{quad}}(W,Z)=\sum_{k=1}^{K}\sum_{i_{k}\in C_{k}}\left[-z_{ki_{k}}w_{k}^{T}x_{i_{k}}+\frac{1}{2}z_{ki_{k}}^{2}\right]\label{eq:EWZ2}
\end{equation}
to be minimized subject to the constraints 

\begin{equation}
\sum_{i_{k}\in C_{k}}z_{ki_{k}}=0,\forall k\label{eq:sumzequalszero}
\end{equation}
in addition to the orthonormal constraints in (\ref{eq:wkwlinner}).
This objective function yields a $Z$ which removes the class-specific
centroid of $C_{k}$ for all classes.

\subsection{Maximizing the absolute value of the inner product}

We have justified our choice of centroid removal mentioned above indirectly
obtained via constraints imposed on Legendre transform auxiliary variables.
The above objective function can be suitably modified when we use
different forms (absolute inner product etc.). To see this, consider
the following objective function which minimizes the negative of the
magnitude of the inner product:
\begin{equation}
E(W)=-\sum_{k=1}^{K}\sum_{i_{k}\in C_{k}}|w_{k}^{T}(x_{i_{k}}-x_{0})|.\label{eq:absvalobj}
\end{equation}
Since $-|x|$ is also a concave function, it too can be majorized.
Consider first replacing the non-differentiable objective function
$-|x|$ with $-\sqrt{x^{2}+\epsilon}$ (also concave) where $\epsilon$
can be chosen to be a suitably small value. Now consider replacing
$-\sqrt{x^{2}+\epsilon}$ with $\min_{y}-xy-\epsilon\sqrt{1-y^{2}}$
which can again quickly checked to be valid for a constrained auxiliary
variable $y\in\left[-1,1\right]$. The constraint is somewhat less
relevant since the minimum w.r.t. $y$ occurs at $y=\frac{x}{\sqrt{x^{2}+\epsilon^{2}}}$
which lies within the constraint interval. Note that this transformation
also yields a linear objective function w.r.t. $x$. As before, we
introduce a new objective function
\begin{equation}
E_{\mathrm{abs}}(W,Z)=\sum_{k=1}^{K}\sum_{i_{k}\in C_{k}}\left[-z_{ki_{k}}w_{k}^{T}x_{i_{k}}-\epsilon\sqrt{1-z_{ki_{k}}^{2}}\right]\label{eq:EWZabs}
\end{equation}
to be minimized subject to the constraints $\sum_{i_{k}\in C_{k}}z_{ki_{k}}=0,\,\forall k$
and $z_{ki_{k}}\in\left[-1,1\right]$ which are the same as in (\ref{eq:sumzequalszero})
in addition to the orthonormal constraints in (\ref{eq:wkwlinner}). 

\subsection{Extension to RKHS kernels}

The generalization to RKHS kernels is surprisingly straightforward.
First, we follow standard kernel PCA and write the weight vector in
terms of the RKHS projected patterns $\phi(x_{l})$ to get 
\begin{equation}
w_{k}=\sum_{i=1}^{N}\alpha_{ki}\phi(x_{i}).\label{eq:wkRKHS}
\end{equation}
Note that the expansion of the weight vector is over all patterns
rather than just the class-specific ones. This assumes that the weight
vector for each class lives in the subspace (potentially infinite
dimensional) spanned by the RKHS projected patterns\textemdash the
same assumption as in standard kernel PCA. The orthogonality constraint
between weight vectors becomes 
\begin{equation}
\begin{array}{ccc}
\langle w_{k},w_{l}\rangle & = & \langle\sum_{i=1}^{N}\alpha_{ki}\phi(x_{i}),\sum_{i=1}^{N}\alpha_{li}\phi(x_{i})\rangle\\
 & = & \sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_{ki}\alpha_{kj}\langle\phi(x_{i}),\phi(x_{j})\rangle\\
 & = & \sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_{ki}\alpha_{kj}K(x_{i},x_{j})
\end{array}\label{eq:wkwlRKHS}
\end{equation}
which is equal to one if $k=l$ and zero otherwise. In matrix form,
the orthonormality constraints become
\begin{equation}
AGA^{T}=I_{K}\label{eq:orthoRKHS}
\end{equation}
where $\left[A\right]_{kl}\equiv\alpha_{ki}$ and $\left[G\right]_{ij}=K(x_{i},x_{j})$
is the well-known Gram matrix of pairwise RKHS inner products between
the patterns. 

The corresponding squared inner product and absolute value of inner
product objective functions are {\small{}
\begin{equation}
E_{\mathrm{Kquad}}(A,Z)=\sum_{k=1}^{K}\sum_{i_{k}\in C_{k}}\left[-\sum_{j=1}^{N}z_{ki_{k}}\alpha_{kj}K(x_{j},x_{i_{k}})+\frac{1}{2}z_{ki_{k}}^{2}\right]\label{eq:Kquadobj}
\end{equation}
}and{\small{}
\begin{equation}
E_{\mathrm{Kabs}}(A,Z)=\sum_{k=1}^{K}\sum_{i_{k}\in C_{k}}\left[-\sum_{j=1}^{N}z_{ki_{k}}\alpha_{kj}K(x_{j},x_{i_{k}})-\epsilon\sqrt{1-z_{ki_{k}}^{2}}\right]\label{eq:Kabsobj}
\end{equation}
}respectively. These have to be minimized w.r.t. the orthonormal constraints
in (\ref{eq:orthoRKHS}) and the origin constraints in (\ref{eq:sumzequalszero}).
Note that the objective functions are identical w.r.t. the matrix
$A$. The parameter $\epsilon$ can be set to a very small but positive
value.
